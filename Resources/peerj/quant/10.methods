In this section we describe our model, the method and the data we are using for our analysis.In order to simulate gene expression data we are using &#xA0;().  is a R package that combines a fuzzy logic with differential equations to enhance the simulation of transcription regulation processes. Differential equations are used to describe the continuous dynamics of gene expression on a continuous time scale and gene-specific kinetic parameters are used to achieve realistic simulations that mimic the real dynamical behavior of gene expression. For our study we are generating gene expression data for three different conditions that correspond to two different types of data: observational gene expression data: normal environmental conditioninterventional gene expression data: growth in rich mediainterventional gene expression data: normal environmental condition interrupted by a positive spike-in stimulation That means, we are generating gene expression data that correspond to observational (I) and interventional data (II and III). However, we are not generating data by gene knockout or silencing&#xA0;(; ). The reason for this is that an inclusion of such perturbation experiments would limit the scope of this paper. Specifically, for human subjects it is for ethical reasons not possible to conduct  gene-knockout experiments. Hence, if we would include such studies we would need to exclude a discussion of gene expression data, e.g.,&#xA0;from clinical studies. On the other hand, the chosen interventional strategies for the generation of the data are equally applicable to model organisms as well as human subjects. This allows a general extrapolation of our results.The first type of data we are generating corresponds to cells in normal environmental conditions meaning that for these simulations we do not use an external stimulation of the gene expression. The second type of data can be seen as a media rich environment which has a favorable effect on the proliferation of cells. For this condition each gene receives an external positive stimulus facilitating its expression. For your simulations this is accomplished by using a constant stimulation of a fixed positive constant . The third type of data corresponds to time dependent interventional data because we alter the environmental condition of the cells over time. This change of the environmental condition translates into a change of the dynamic of the gene expression in a time dependent manner. Specifically, we start simulating gene expression under the same conditions as in (I) but add at a certain time point, , a constant but random stimulation &#xD7; for each gene. Here  is a constant factor and  is a random variable uniformly sampled from [0,1]. This stimulation lasts a short period of time &#x394;&#xA0;=&#xA0;0.2. After this period, the gene expression is again governed by the same conditions as in (I). Biologically, this corresponds to a normal condition that is interrupted by a short positive stimulation, e.g.,&#xA0;the administration of a drug.We are conducting our analysis for two different topology types of regulatory networks that govern the interactions between genes. The first type is a Erd&#xF6;s&#x2013;R&#xE9;yni network (; ) that is generated by an algorithm. This network represents a synthetic network. The second type is a subnetwork of the transcriptional regulatory network of &#xA0;() and, hence, represents a real biological network. Each of these networks consists of 100 genes.For each of these two types of regulatory networks we are generating simulated gene expression data, as described in the previous section. This allows us to study the influence that the interaction structure among the genes has on the performance of inference algorithms by keeping the dynamical system of the underlying equations unchanged.In  we show a schematic overview of our simulation study. For the generation of gene expression data we are using , which simulates coupled systems of differential equations. The coupling between the genes is given by a network . The connections between two genes can be positive (activator) or negative (repressor) and, hence, lead to the enhancement or repression of a transcription regulation.We use  to generate time series data that are measured at  different time points, i.e, {,&#x2026;,}. We are not using the time series data themselves to estimate the underlying network, given by , but, instead, we generate an ensemble of &#xD7;&#xD7; different data sets. We organize these data sets according to the observation time points, i.e.,&#xA0;&#x1D49F;&#xA0;=&#xA0;{(),(),&#x2026;,()} with &#x2208;{1,&#x2026;,}. This gives us  different sets of data sets, &#x1D49F;, each consisting of  different data sets () with &#x2208;{1,&#x2026;,} and &#x2208;{1,&#x2026;,} with  samples. That means, each data set () contains measurements that correspond to one particular time point  only. See  for an overview.These sets of data sets, &#x1D49F;, allow us to assess the inference characteristics of statistical network inference methods on the population level, because when the value of  is large enough chosen it allow us to draw conclusions with respect to the behavior of the population. Specifically, we use each of the  data sets () in &#x1D49F; to infer  networks, {(),&#x2026;,()}. By using knowledge about the true underlying network structure among the genes, given by , we obtain  different F-scores that quantify the inference performance of the used network estimation algorithm, i.e.,&#xA0;{(),&#x2026;,()}. Now the ensemble of F-scores allows us to estimate the mean inference performance and its variability. It is important to emphasize that information about the variability of the inference performance is necessary in order to obtain a robust evaluation. If only one or a few data sets would be used, the obtained results could be spurious. To avoid this, we use for our following numerical analysis &#xA0;=&#xA0;100, &#xA0;=&#xA0;11 and a sample size of &#xA0;=&#xA0;300. This results in a total of &#xD7;&#xA0;=&#xA0;1100 different data sets for each network  and each condition. Application of 5 different inference methods results in the inference of 5500 networks for each network  and each condition. In total, we infer for the two different networks we are studying (Erd&#xF6;s&#x2013;R&#xE9;yni network and subnetwork of the transcriptional regulatory network of ) and the five different inference methods (Aracne, BC3NET, CLR, C3NET and MRNET) 33,000 different networks.The above procedure is repeated for each environmental condition and each  regulatory network studied.In order to evaluate the performance of a network inference algorithm we are using the F-score. The F-score is defined by  and assumes values in [0,1], whereas zero corresponds to the worst and one to the best performance. Here  corresponds to the  and  to the , i.e.,&#xA0;   
                The precision and recall are functions of the number of true positives (TP), false positives (FP) and false negatives (FN). We would like to emphasize that these numbers are available from the comparison of the estimated network, , with the true network, . More precisely, for an estimated network, , the true network, , and their corresponding adjacency matrices, , and, , we obtain   
                 
               
            Here () corresponds to the indicator function that is 1 if its argument is true and 0 otherwise.For our numerical analysis to infer gene regulatory networks, we use 5 different network inference methods, BC3NET, C3NET, CLR, MRNET and Aracne. In  we provide a summary of these methods. A detailed discussion of the functioning of these methods can be found in&#xA0;, , , , ,  or in a recent review paper&#xA0;().All 5 methods are information theory based utilizing estimates of mutual information coefficients&#xA0;(). Mutual information coefficients form a non-linear extension of (linear) correlation coefficients, e.g.,&#xA0;the Pearson correlation coefficient. Mutual information is defined by the marginal probabilities () and () and the joint probability (,) of two random variables  and &#xA0;():  Here log means the logarithm to the base of 2. The mutual information, (,), between two random variables has the property to be always .&#xA0;(,) is equal to zero if the two random variables are (statistically) independent from each other, because in this case (,)&#xA0;=&#xA0;()().Summary of the 5 network inference methods we use for our analysis. The first column gives the name of the method, the second provides a succinct description of the principle idea the method is based on and column three gives references describing the methods in detail.Practically, the marginal and joint probability distributions are not available and, hence, mutual information values need to be estimated by means of statistical methods from the data. In&#xA0; it was found that the Miller&#x2013;Madow estimator&#xA0;() has overall the most favorable inference capabilities compared with 3 further esimators.The Miller&#x2013;Madow estimator utilizes the fact that the mutual information can also be written in terms of entropies&#xA0;(),  Here the entropy for a random variable  is defined by:  and the joint entropy (,) is given by  The simplest estimator to estimate such entropies is the empirical estimator that estimates the entropy from the observed joint frequencies for each bin&#xA0;(). Specifically, the empirical entropy  can be estimated from the observed frequency distribution with  number of samples in bin , the total number of samples  and the total number of bins . For example, for the entropy in  the empirical estimator is given by,  The Empirical estimator gives the maximum-likelihood entropy estimate for a discretized random variable. A main problem of the empirical approach is the underestimation of the true entropy, , due to an undersampling of the cell frequencies when the number of bins increases. A variety of approaches have been developed to account for this bias that range from correcting the estimate by a constant factor or using a multinomial distribution to model the extend of missing information.The Miller&#x2013;Madow estimator&#xA0;() accounts for the undersampling bias by adjusting the estimate by a constant factor that is proportional to the bin size and the sample size:  Here  is the number of bins and  is the number of samples.A practical problem when applying the Miller&#x2013;Madow estimator is that it is computationally demanding, .e.g.,&#xA0;compared to the Pearson estimator for mutual information&#xA0;(). The Pearson estimator for mutual information is estimated from  where &#x3C1;(,) is the Pearson correlation coefficient. For normal distributed random variables  and  this expression is exact.From a numerical comparison of both estimators we find that the application of the Miller&#x2013;Madow estimator takes about two orders of magnitude longer than the application of the Pearson estimator for mutual information. Further, from comparing different network inference methods we find that the performance for all methods is similarly effected by the estimators. For reasons of computational ease, we use for our following simulations the Pearson estimator, because our principle results are independent of the selected estimator and do not depend on the selection of the best estimator leading to the highest F-scores.